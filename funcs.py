import matplotlib.pyplot as plt
import autograd.numpy as np
from scipy.io import loadmat
from scipy.stats import zscore

import pymanopt
from pymanopt.manifolds import Stiefel
from pymanopt.optimizers import TrustRegions


# manifold optimization to two orthogonal subspaces that maximize the sum of variance captured.
# Written by Xiyuan Jiang and Hemant Saggar
# Date: 06/27/2020
# Ported to python by Munib Hasnain 
# Date: 2023-07-20

def create_cost_and_egrad(manifold, eigvalsNull, eigvalsPotent, dNull, dPotent, Ppotent, Pnull, covNull, covPotent):
    # returns function references for the cost function and the euclidean gradient of the cost function
    
    # @pymanopt.function.numpy(manifold)
    @pymanopt.function.autograd(manifold)
    def cost(Q):
        Qpotent = Q @ Ppotent
        Qnull = Q @ Pnull
        normPotent = np.sum(eigvalsPotent[1:dPotent])
        normNull = np.sum(eigvalsNull[1:dNull])
        return -0.5 * np.trace(Qpotent.T @ covPotent @ Qpotent) / normPotent - 0.5 * np.trace(Qnull.T @ covNull @ Qnull) / normNull


    # @pymanopt.function.numpy(manifold)
    @pymanopt.function.autograd(manifold)
    def euclidean_gradient(Q):
        normPotent = np.sum(eigvalsPotent[1:dPotent])
        normNull = np.sum(eigvalsNull[1:dNull])
        return -covPotent @ Q @ (Ppotent@Ppotent.T) / normPotent - covNull @ Q @ (Pnull@Pnull.T) / normNull

    return cost, euclidean_gradient


def orthogonal_subspaces(nNeurons, dNull, dPotent, covNull, covPotent):
    # returns:
    # Q, matrix of size (nNeurons,nDimensions), where nDimensions = dNull + dPotent
    #   Q contains the dimensions corresponding to both null and potent subspaces. 
    #   The first dPotent columns corresponds to the potent subspace, and the last dNull columns the null subspace
    # Ppotent, Q * Ppotent = Qpotent
    # Pnull,   Q * Pnull = Qnull
    
    
    optimizer = TrustRegions(verbosity=0)
    manifold = Stiefel(nNeurons, dNull + dPotent)

    # ensure covariance matrices are symmetric
    assert (covNull == covNull.T).all(), "covNull is not symmetric"
    assert (covPotent == covPotent.T).all(), "covPotent is not symmetric"
    # ensure sizes of covariance matrices are equal
    assert (covNull.shape == covPotent.shape), "covNull and covPotent have different shapes"

    # get eigenvalues is descending order of magnitude
    eigvalsPotent, _ = np.linalg.eig(covPotent) # hoping that these all eigs are positive, still only divinding by largest algebraic +ve values
    eigvalsNull, _ = np.linalg.eig(covNull)
    assert ~(eigvalsPotent<0).any(), 'eigvalsPotent <0'
    assert ~(eigvalsNull<0).any(), 'eigvalsNull <0'
    Ppotent = np.concatenate((np.eye(dPotent), np.zeros((dNull,dPotent))), axis=0)
    Pnull = np.concatenate((np.zeros((dPotent,dNull)), np.eye(dNull)), axis=0)

    # get cost and euclidean gradient function references
    cost, egrad = create_cost_and_egrad(manifold, eigvalsNull, eigvalsPotent, dNull, dPotent, Ppotent, Pnull, covNull, covPotent)

    # set up optimization problem 
    problem = pymanopt.Problem(
        manifold,
        cost,
        euclidean_gradient = egrad,
        euclidean_hessian = None,
    )

    # solve
    Q = optimizer.run(problem).point
    
    return Q, Ppotent, Pnull